# A Simplified Implementation of FlashAttention in CUDA

This is the repository for my senior design project, focusing on a
high-performance implementation of a memory-aware attention mechanism for
Transformer models.

The goal is to create a simplified version of the FlashAttention forward pass in
C++ and CUDA, demonstrating the performance benefits of IO-aware algorithms on
GPU hardware.
